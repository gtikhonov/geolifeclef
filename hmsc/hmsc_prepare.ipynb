{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00b6e52-c89c-4af0-9e54-3d57140eb99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as v2\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
    "from torchinfo import summary\n",
    "\n",
    "from terratorch.models.pixel_wise_model import freeze_module\n",
    "from huggingface_hub import hf_hub_download\n",
    "from terratorch.models.backbones.prithvi_mae import PrithviViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32985305-e675-470a-a75e-e4938e10cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed) # Set seed for Python's built-in random number generator\n",
    "    np.random.seed(seed) # Set seed for numpy\n",
    "    if torch.cuda.is_available(): # Set seed for CUDA if available\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba51b3fe-b184-44a8-9c9a-ebc8b62d9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raster(path, if_img=1, crop=None):\n",
    "  with rasterio.open(path) as src:\n",
    "    img = src.read(out_dtype=np.float32)\n",
    "    if if_img==1:\n",
    "      bands=[0,1,2,3]\n",
    "      img = img[bands,:,:]\n",
    "    if crop:\n",
    "      img = img[:, -crop[0]:, -crop[1]:]\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dec30b7-cab6-4ec2-8540-2ee99b8d6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dir_sentinel, dir_landsat, dir_bioclim, metadata, cov_columns, subset, num_classes=None, transform_sentinel=None, transform_landsat=None):\n",
    "        # transform_landsat corresponds to landsat and bioclim cubes combined\n",
    "        self.subset = subset\n",
    "        self.transform_sentinel = transform_sentinel\n",
    "        self.std_sentinel = np.array(std_sentinel)\n",
    "        self.transform_landsat = transform_landsat\n",
    "        self.dir_sentinel = dir_sentinel\n",
    "        self.dir_landsat = dir_landsat\n",
    "        self.dir_bioclim = dir_bioclim\n",
    "        self.metadata = metadata\n",
    "        self.cov_columns = cov_columns\n",
    "        self.num_classes = num_classes\n",
    "        if self.subset == \"test\":\n",
    "            self.landsat_file_sep = \"_\"\n",
    "        elif self.subset == \"train\":\n",
    "            self.landsat_file_sep = \"-\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        cov = torch.tensor(self.metadata.loc[idx, self.cov_columns].values.astype(np.float32))\n",
    "        lonlat = torch.tensor(self.metadata.loc[idx, [\"lon\",\"lat\"]].values.astype(np.float32))\n",
    "        path_landsat = os.path.join(self.dir_landsat, f\"GLC25-PA-{self.subset}-landsat{self.landsat_file_sep}time{self.landsat_file_sep}series_{survey_id}_cube.pt\")\n",
    "        sample_landsat = torch.nan_to_num(torch.load(path_landsat, weights_only=True)[:,:,:landsat_year_len])\n",
    "        path_bioclim = os.path.join(self.dir_bioclim, f\"GLC25-PA-{self.subset}-bioclimatic_monthly_{survey_id}_cube.pt\")\n",
    "        sample_bioclim = torch.nan_to_num(torch.load(path_bioclim, weights_only=True))\n",
    "        tmp1 = torch.reshape(sample_bioclim, [4,-1])\n",
    "        tmp2 = torch.reshape(torch.cat([tmp1[:,:1], tmp1[:,:bioclim_month_len]], axis=-1), [4,landsat_year_len,4,3])\n",
    "        sample_bioclim_new = torch.permute(torch.mean(tmp2, -1), [0,2,1])[:2] \n",
    "        sample_landsat = torch.cat([sample_landsat, sample_bioclim_new], 0)\n",
    "        if self.transform_landsat:\n",
    "            sample_landsat = self.transform_landsat(sample_landsat)\n",
    "        sample_landsat = torch.mean(sample_landsat, [-1])\n",
    "        dir1, dir2 = str(survey_id)[-2:], str(survey_id)[-4:-2]        \n",
    "        path_sentinel = os.path.join(self.dir_sentinel, dir1, dir2, f\"{survey_id}.tiff\")\n",
    "        sample_sentinel = torch.nan_to_num(torch.from_numpy(load_raster(path_sentinel)).to(torch.float32))\n",
    "        if self.transform_sentinel:\n",
    "            sample_sentinel = self.transform_sentinel(sample_sentinel)\n",
    "        sample_sentinel = torch.mean(sample_sentinel, [-1, -2])\n",
    "        return sample_sentinel, sample_landsat, cov, lonlat, survey_id\n",
    "\n",
    "class TrainDataset(TestDataset):\n",
    "  def __init__(self, dir_sentinel, dir_landsat, dir_bioclim, metadata, cov_columns, label_dict, subset, num_classes, transform_sentinel=None, transform_landsat=None,\n",
    "              mean_sentinel=np.array([0.0]), std_sentinel=np.array([1.0])):\n",
    "    super(TrainDataset, self).__init__(dir_sentinel, dir_landsat, dir_bioclim, metadata, cov_columns, subset, num_classes, transform_sentinel, transform_landsat)\n",
    "    self.label_dict = label_dict\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample_sentinel, sample_landsat, cov, lonlat, survey_id = super(TrainDataset, self).__getitem__(idx)\n",
    "    species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "    label = torch.zeros(self.num_classes).scatter(0, torch.tensor(species_ids), torch.ones(len(species_ids)))\n",
    "    return sample_sentinel, sample_landsat, cov, lonlat, label, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3650979c-6ecb-48d3-835d-14238b9f597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 6\n",
    "pa_presence_threshold = 1\n",
    "num_classes_total = 11255\n",
    "landsat_year_len = 18\n",
    "bioclim_month_len = landsat_year_len*12-1\n",
    "validation_prop = 0.1\n",
    "sel_countries = [\"France\", \"Denmark\", \"Netherlands\", \"Italy\"] #, \"Austria\"\n",
    "cov_countries = 1\n",
    "cov_area, cov_elevation, cov_snow = 1, 1, 1\n",
    "cov_soil, cov_worldcover, cov_landcover = 1, 1, 1\n",
    "path_data = \"/home/gt/DATA/geolifeclef-2025\"\n",
    "\n",
    "mean_landsat = 1*np.array([ 15.0698,   16.0923,    7.9312,   68.9794,   47.9505,   24.8804, 7089.4349, 2830.6658])\n",
    "std_landsat =  1*np.array([ 11.7218,   10.2417,    9.6499,   18.7112,   13.1681,    9.2436, 3332.3618,   56.7270])\n",
    "mean_sentinel = 1*np.array([ 624.8547,  684.7646,  456.7674, 2924.1753])\n",
    "std_sentinel =  1*np.array([ 416.0408,  351.1005,  315.8956,  943.6141])\n",
    "\n",
    "class HorizontalCycleTransform(torch.nn.Module):\n",
    "    def forward(self, img):\n",
    "        img2 = torch.cat([img, img], -1)\n",
    "        start = torch.randint(img.shape[-1], (1,))[0]\n",
    "        new_img = img2[:,:,start:start+img.shape[-1]]\n",
    "        return new_img\n",
    "\n",
    "transform_landsat = v2.Compose([\n",
    "    v2.Normalize(mean_landsat, std_landsat),\n",
    "])\n",
    "transform_landsat_test = v2.Compose([\n",
    "    v2.Normalize(mean_landsat, std_landsat),\n",
    "])\n",
    "transform_sentinel = v2.Compose([\n",
    "    v2.Normalize(mean_sentinel, std_sentinel),\n",
    "])\n",
    "transform_sentinel_test = v2.Compose([\n",
    "    v2.Normalize(mean_sentinel, std_sentinel)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a48459-4d2c-46e1-8574-04cfc3384879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['areaLog', 'Elevation', 'conFra', 'conDen', 'conNet', 'conIta', 'conOther', 'Soilgrid-bdod', 'Soilgrid-cec', 'Soilgrid-cfvo', 'Soilgrid-clay', 'Soilgrid-nitrogen', 'Soilgrid-phh2o', 'Soilgrid-sand', 'Soilgrid-silt', 'Soilgrid-soc', 'wc_10', 'wc_20', 'wc_30', 'wc_40', 'wc_50', 'wc_60', 'wc_80', 'wc_90', 'LandCover-1', 'LandCover-3', 'LandCover-4', 'LandCover-6', 'LandCover-9', 'LandCover-12', 'LandCover-13', 'scd']\n",
      "All rows match:  [True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "train_path_sentinel = os.path.join(path_data, \"SatelitePatches/PA-train\")\n",
    "train_path_landsat = os.path.join(path_data, \"SateliteTimeSeries-Landsat/cubes/PA-train\")\n",
    "train_path_bioclim = os.path.join(path_data, \"BioclimTimeSeries/cubes/PA-train\")\n",
    "train_metadata = pd.read_csv(os.path.join(path_data, \"GLC25_PA_metadata_train.csv\"))\n",
    "train_metadata = train_metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "train_metadata['speciesId'] = train_metadata['speciesId'].astype(int)\n",
    "train_metadata[\"speciesIdOrig\"] = train_metadata['speciesId']\n",
    "tmp = train_metadata[\"speciesId\"].value_counts() >= pa_presence_threshold\n",
    "train_metadata.loc[~train_metadata[\"speciesId\"].isin(tmp[tmp].index), \"speciesId\"] = -1\n",
    "sp_categorical = train_metadata[\"speciesId\"].astype(\"category\").values\n",
    "num_classes = len(sp_categorical.categories)\n",
    "train_metadata['speciesId'] = sp_categorical.codes\n",
    "\n",
    "tmp = train_metadata.groupby(\"surveyId\").agg({\"surveyId\":\"first\", \"lat\":\"first\", \"lon\":\"first\", \"areaInM2\":lambda x: list(x.unique()), \"region\":\"first\", \"country\":\"first\", \"speciesId\":list})\n",
    "train_label_series = tmp.set_index(\"surveyId\").speciesId\n",
    "train_metadata = tmp.drop(columns=[\"speciesId\"]).set_index(\"surveyId\", drop=False)\n",
    "train_metadata[\"area\"] = train_metadata[\"areaInM2\"].apply(lambda x: 1.0 if np.isinf(x).all() else np.mean(x, where=~np.isinf(x)))\n",
    "train_metadata[\"areaLog\"] = np.log10(train_metadata[\"area\"])\n",
    "\n",
    "train_metadata['area'].fillna(train_metadata['area'].mean(), inplace=True)\n",
    "train_metadata['areaLog'].fillna(train_metadata['areaLog'].mean(), inplace=True)\n",
    "country_columns = [\"con\"+country[:3] for country in sel_countries] + [\"conOther\"]\n",
    "for country, col in zip(sel_countries, country_columns[:-1]):\n",
    "    train_metadata[col] = train_metadata[\"country\"] == country\n",
    "train_metadata[country_columns[-1]] = ~train_metadata[\"country\"].isin(sel_countries)\n",
    "train_elevation = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"Elevation\", \"GLC25-PA-train-elevation.csv\"), index_col=0)\n",
    "train_elevation['Elevation'].fillna((train_elevation['Elevation'].mean()), inplace=True)\n",
    "train_soil = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"SoilGrids\", \"GLC25-PA-train-soilgrids.csv\"), index_col=0)\n",
    "for column in train_soil.columns: train_soil[column].fillna((train_soil[column].mean()), inplace=True)\n",
    "train_worldcover = pd.read_csv(os.path.join(path_data, \"worldcover\", \"s2_pa_train_survey_points_with_worldcover.csv\"), index_col=0)\n",
    "train_wcdummy = pd.get_dummies(train_worldcover[\"class\"], prefix=\"wc\")\n",
    "train_wcdummy.drop(columns=\"wc_70\", inplace=True)\n",
    "train_wcdummy.drop(columns=\"wc_100\", inplace=True)\n",
    "train_landcover = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"LandCover\", \"GLC25-PA-train-landcover.csv\"), index_col=0)\n",
    "landcover_col_ind = [0,2,3,5,8,11,12]\n",
    "train_landcover = train_landcover.iloc[:, landcover_col_ind]\n",
    "train_snow = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"chelsa_snow\", \"pa_train_snowcover_chelsa_scd.csv\"), index_col=0).sort_index()\n",
    "\n",
    "cov_flag_list = [cov_area, cov_elevation, cov_countries, cov_soil, cov_worldcover, cov_landcover, cov_snow]\n",
    "cov_name_list = [[\"areaLog\"], [\"Elevation\"], country_columns, list(train_soil.columns), list(train_wcdummy.columns), list(train_landcover.columns), list(train_snow.columns)]\n",
    "cov_columns = sum([flag*name for flag, name in zip(cov_flag_list, cov_name_list)], [])\n",
    "print(cov_columns)\n",
    "train_df_list = [train_metadata, train_elevation, train_soil, train_wcdummy, train_landcover, train_snow]\n",
    "print(\"All rows match: \", [(train_df_list[0].index==df.index).all() for df in train_df_list[1:]])\n",
    "train_combined = pd.concat(train_df_list, axis=1)\n",
    "cov_norm_coef = train_combined.loc[:,cov_columns].agg(['mean', 'std'])\n",
    "dummy_columns = country_columns + list(train_wcdummy.columns)\n",
    "cov_norm_coef.loc[\"mean\",dummy_columns] = 0\n",
    "cov_norm_coef.loc[\"std\",dummy_columns] = 1\n",
    "train_combined.loc[:,cov_columns] = (train_combined.loc[:,cov_columns] - cov_norm_coef.loc[\"mean\"]) / cov_norm_coef.loc[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3673f3d-279c-4c46-b4e9-4f17df64731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4]) torch.Size([8, 4]) torch.Size([32]) torch.Size([2]) torch.Size([5016])\n"
     ]
    }
   ],
   "source": [
    "train_data = train_combined.reset_index(drop=True)\n",
    "train_label_dict = train_label_series.to_dict()\n",
    "train_dataset = TrainDataset(train_path_sentinel, train_path_landsat, train_path_bioclim, train_data, cov_columns, train_label_dict, \n",
    "                             subset=\"train\", num_classes=num_classes, transform_sentinel=transform_sentinel, transform_landsat=transform_landsat)\n",
    "print(train_dataset[0][0].shape, train_dataset[0][1].shape, train_dataset[0][2].shape, train_dataset[0][3].shape, train_dataset[0][4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd017612-5b12-494d-9c12-ea6eb5c06e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16053792b74b4884bc8e2299c11ed570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cov_list, y_list = [], []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    vals = train_dataset[i]\n",
    "    cov = torch.concat([vals[0], vals[1].flatten(), vals[2], vals[3]]).numpy()\n",
    "    cov_list.append(cov)\n",
    "    y_list.append(vals[4].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e883146a-7811-4a43-84a7-30d4159dd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f\"sentinel{i}\" for i in range(len(vals[0]))] + [f\"landsatbio{i}{j}\" for i in range(vals[1].shape[0]) for j in range(vals[1].shape[1])] + cov_columns + [\"lon\",\"lat\"]\n",
    "cov = pd.DataFrame(cov_list, columns=cols)\n",
    "Y = pd.DataFrame(y_list).astype(int)\n",
    "os.makedirs(os.path.join(path_data, \"hmsc\", \"\"), exist_ok=True)\n",
    "cov.to_csv(os.path.join(path_data, \"hmsc\", \"train_cov.csv\"), index_label=False)\n",
    "Y.to_csv(os.path.join(path_data, \"hmsc\", \"train_Y.csv\"), index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37652cd9-388c-4734-a3aa-d2c7ca75e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows match:  [True, True, True, True, True]\n",
      "torch.Size([4]) torch.Size([8, 4]) torch.Size([32]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "test_path_sentinel = os.path.join(path_data, \"SatelitePatches/PA-test\")\n",
    "test_path_landsat = os.path.join(path_data, \"SateliteTimeSeries-Landsat/cubes/PA-test\")\n",
    "test_path_bioclim = os.path.join(path_data, \"BioclimTimeSeries/cubes/PA-test\")\n",
    "test_metadata = pd.read_csv(os.path.join(path_data, \"GLC25_PA_metadata_test.csv\")).set_index(\"surveyId\", drop=False).sort_index()\n",
    "test_metadata.rename(columns={\"areaInM2\": \"area\"}, inplace=True)\n",
    "test_metadata.replace({\"area\": [np.inf, -np.inf]}, 1.0, inplace=True)\n",
    "test_metadata['areaLog'] = np.log10(test_metadata['area'])\n",
    "test_metadata['area'].fillna(test_metadata['area'].mean(), inplace=True)\n",
    "test_metadata['areaLog'].fillna(test_metadata['areaLog'].mean(), inplace=True)\n",
    "for country, col in zip(sel_countries, country_columns[:-1]):\n",
    "    test_metadata[col] = test_metadata[\"country\"] == country\n",
    "test_metadata[country_columns[-1]] = ~test_metadata[\"country\"].isin(sel_countries)\n",
    "test_elevation = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"Elevation\", \"GLC25-PA-test-elevation.csv\"), index_col=0).sort_index()\n",
    "test_elevation = test_elevation.loc[test_elevation.index.isin(test_metadata.index)]\n",
    "test_elevation['Elevation'].fillna((test_elevation['Elevation'].mean()), inplace=True)\n",
    "test_soil = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"SoilGrids\", \"GLC25-PA-test-soilgrids.csv\"), index_col=0).sort_index()\n",
    "test_soil = test_soil.loc[test_soil.index.isin(test_metadata.index)]\n",
    "for column in test_soil.columns: test_soil[column].fillna((test_soil[column].mean()), inplace=True)\n",
    "test_worldcover = pd.read_csv(os.path.join(path_data, \"worldcover\", \"pa_test_survey_points_with_worldcover.csv\"), index_col=0).sort_index()\n",
    "test_wcdummy = pd.get_dummies(test_worldcover[\"class\"], prefix=\"wc\")\n",
    "test_wcdummy.drop(columns=\"wc_100\", inplace=True)\n",
    "# test_wcdummy.insert(6, \"wc_70\", False)\n",
    "test_landcover = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"LandCover\", \"GLC25-PA-test-landcover.csv\"), index_col=0).sort_index()\n",
    "test_landcover = test_landcover.loc[test_landcover.index.isin(test_metadata.index)]\n",
    "test_landcover = test_landcover.iloc[:, landcover_col_ind]\n",
    "test_snow = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"chelsa_snow\", \"pa_test_snowcover_chelsa_scd.csv\"), index_col=0).sort_index()\n",
    "test_snow = test_snow.loc[test_snow.index.isin(test_metadata.index)]\n",
    "\n",
    "test_df_list = [test_metadata, test_elevation, test_soil, test_wcdummy, test_landcover, test_snow]\n",
    "print(\"All rows match: \", [(test_df_list[0].index==df.index).all() for df in test_df_list[1:]])\n",
    "test_combined = pd.concat(test_df_list, axis=1)\n",
    "test_combined.loc[:,cov_columns] = (test_combined.loc[:,cov_columns] - cov_norm_coef.loc[\"mean\"]) / cov_norm_coef.loc[\"std\"]\n",
    "test_combined.reset_index(drop=True, inplace=True)\n",
    "test_dataset = TestDataset(test_path_sentinel, test_path_landsat, test_path_bioclim, test_combined, cov_columns, subset=\"test\", \n",
    "                           transform_sentinel=transform_sentinel_test, transform_landsat=transform_landsat_test)\n",
    "print(test_dataset[0][0].shape, test_dataset[0][1].shape, test_dataset[0][2].shape, test_dataset[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51acac2c-b672-4059-a98e-d0ebda9cf6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72e3a1ef1af401aa0271ae3f00bf445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cov_list = []\n",
    "for i in tqdm(range(len(test_dataset))):\n",
    "    vals = test_dataset[i]\n",
    "    cov = torch.concat([vals[0], vals[1].flatten(), vals[2], vals[3]]).numpy()\n",
    "    cov_list.append(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bdefbfc-c3f1-40a8-8263-75836e33f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f\"sentinel{i}\" for i in range(len(vals[0]))] + [f\"landsatbio{i}{j}\" for i in range(vals[1].shape[0]) for j in range(vals[1].shape[1])] + cov_columns + [\"lon\",\"lat\"]\n",
    "cov = pd.DataFrame(cov_list, columns=cols)\n",
    "os.makedirs(os.path.join(path_data, \"hmsc\"), exist_ok=True)\n",
    "cov.to_csv(os.path.join(path_data, \"hmsc\", \"test_cov.csv\"), index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e459331-46e4-4ef4-8064-ce2f655f1606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
