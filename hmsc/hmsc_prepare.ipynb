{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b6e52-c89c-4af0-9e54-3d57140eb99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as v2\n",
    "from terratorch.models.backbones.prithvi_mae import PrithviViT\n",
    "sys.path.append('../prithvi/')\n",
    "from utils import set_seed\n",
    "from glc_datasets import TrainDataset, TestDataset, read_train_data, read_test_data\n",
    "from models import ModifiedPrithviResNet18, prithvi_terratorch\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "os.listdir(os.environ['LOCAL_SCRATCH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca26537-b77f-41fa-b22d-c5b500df00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_type = \"deep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650979c-6ecb-48d3-835d-14238b9f597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 6\n",
    "pa_presence_threshold = 1\n",
    "num_classes_total = 11255\n",
    "landsat_year_len = 18\n",
    "validation_prop = 0.1\n",
    "sel_countries = [\"France\", \"Denmark\", \"Netherlands\", \"Italy\"] #, \"Austria\"\n",
    "cov_countries = 1\n",
    "cov_area, cov_elevation, cov_snow = 1, 1, 1\n",
    "cov_soil, cov_worldcover, cov_landcover = 1, 1, 1\n",
    "if prepare_type == \"deep\":\n",
    "    cov_snow = cov_landcover = 0\n",
    "if os.environ[\"HOSTNAME\"] == \"gtbase\":\n",
    "    path_save = path_data = \"/home/gt/DATA/geolifeclef-2025\"\n",
    "    print(\"local, using\", f\"path_data=path_save={path_data}\")\n",
    "else:\n",
    "    path_data = os.environ['LOCAL_SCRATCH']\n",
    "    path_save = os.environ['GLC_SCRATCH']\n",
    "    print(\"mahti, using\", f\"path_data={path_data};\", f\"path_save={path_save}\")\n",
    "\n",
    "mean_landsat = 1*np.array([ 15.0698,   16.0923,    7.9312,   68.9794,   47.9505,   24.8804, 7089.4349, 2830.6658])\n",
    "std_landsat =  1*np.array([ 11.7218,   10.2417,    9.6499,   18.7112,   13.1681,    9.2436, 3332.3618,   56.7270])\n",
    "mean_sentinel = 1*np.array([ 624.8547,  684.7646,  456.7674, 2924.1753])\n",
    "std_sentinel =  1*np.array([ 416.0408,  351.1005,  315.8956,  943.6141])\n",
    "\n",
    "transform_landsat = v2.Compose([\n",
    "    v2.Normalize(mean_landsat, std_landsat),\n",
    "])\n",
    "transform_sentinel = v2.Compose([\n",
    "    v2.Normalize(mean_sentinel, std_sentinel),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cf6e6-8f0f-4ac5-8e0e-8bb5674df3e5",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d3433-1cd5-4569-bdfe-8e66cac3600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare_type == \"orig\":\n",
    "    image_mean = True\n",
    "    sentinel_mask_channel = False\n",
    "elif prepare_type == \"deep\":\n",
    "    image_mean = False\n",
    "    sentinel_mask_channel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc4eb2-56b1-4d3a-bef3-49b15399d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_flag_list = [cov_area, cov_elevation, cov_countries, cov_soil, cov_worldcover, cov_landcover, cov_snow]\n",
    "train_combined, train_label_series, cov_columns, cov_norm_coef, num_classes = read_train_data(path_data, cov_flag_list, sel_countries, pa_presence_threshold)\n",
    "#cov_norm_coef.to_csv(os.path.join(path_data, \"hmsc\", \"train_cov_mean_std.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3673f3d-279c-4c46-b4e9-4f17df64731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "train_path_sentinel = os.path.join(path_data, \"SatelitePatches/PA-train\")\n",
    "train_path_landsat = os.path.join(path_data, \"SateliteTimeSeries-Landsat/cubes/PA-train\")\n",
    "train_path_bioclim = os.path.join(path_data, \"BioclimTimeSeries/cubes/PA-train\")\n",
    "train_data = train_combined.reset_index(drop=True)\n",
    "train_label_dict = train_label_series.to_dict()\n",
    "train_dataset = TrainDataset(train_path_sentinel, train_path_landsat, train_path_bioclim, train_data, cov_columns, train_label_dict, \n",
    "                             subset=\"train\", num_classes=num_classes, transform_sentinel=transform_sentinel, transform_landsat=transform_landsat,\n",
    "                            landsat_year_len=landsat_year_len, image_mean=image_mean, sentinel_mask_channel=sentinel_mask_channel)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "print(train_dataset[0][0].shape, train_dataset[0][1].shape, train_dataset[0][2].shape, train_dataset[0][3].shape, train_dataset[0][4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b90b80-4f2c-47e2-9cb0-0f9d34822b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare_type == \"deep\":\n",
    "    patch_size = [1,16,16]\n",
    "    n_frame = 1\n",
    "    n_channel = 5\n",
    "    embed_dim = 1024\n",
    "    decoder_depth = 8\n",
    "    num_heads = 16\n",
    "    mlp_ratio = 4\n",
    "    resnet_dim = 1000\n",
    "    hidden_last_dim = 1000 + 128\n",
    "    head_dropout = 0.0\n",
    "    \n",
    "    prithvi_instance = PrithviViT(\n",
    "            patch_size=patch_size,\n",
    "            num_frames=n_frame,\n",
    "            in_chans=n_channel,\n",
    "            embed_dim=embed_dim,\n",
    "            decoder_depth=decoder_depth,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            head_dropout=head_dropout,\n",
    "            backbone_input_size=[1,64,64],\n",
    "            encoder_only=False,\n",
    "            padding=True,\n",
    "    )\n",
    "    prithvi_model = prithvi_terratorch(None, prithvi_instance, [1,64,64])\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"DEVICE = CUDA\")\n",
    "    prithvi_model.to(device)\n",
    "    \n",
    "    model = ModifiedPrithviResNet18(num_classes, len(cov_columns), resnet_dim, hidden_last_dim, prithvi_model).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(path_data, \"hmsc\", \"0507_135023_weights75.pth\"), weights_only=True, map_location=device))\n",
    "    model.fc_final = nn.Identity()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd017612-5b12-494d-9c12-ea6eb5c06e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_list, cov_list, y_list = [], [], []\n",
    "with torch.no_grad():\n",
    "    for sentinel, landsat, data_cov, lonlat, label, survey in tqdm(train_loader):\n",
    "        if prepare_type == \"orig\":\n",
    "            cov = torch.concat([sentinel, landsat.reshape([landsat.shape[0], -1]), data_cov, lonlat], dim=1).numpy()\n",
    "        elif prepare_type == \"deep\":\n",
    "            sentinel = sentinel.to(device)[:,:,None,:,:]\n",
    "            landsat, data_cov, lonlat = landsat.to(device), data_cov.to(device), lonlat.to(device)\n",
    "            features = model(sentinel, landsat, data_cov, lonlat)\n",
    "            cov = torch.concat([features, lonlat], dim=1).cpu().numpy()\n",
    "        cov_list.append(cov)\n",
    "        y_list.append(label.numpy())\n",
    "        surv_list.append(survey.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883146a-7811-4a43-84a7-30d4159dd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare_type == \"orig\":\n",
    "    cols = [f\"sentinel{i}\" for i in range(sentinel.shape[-1])] + [f\"landsatbio{i}{j}\" for i in range(landsat.shape[-2]) for j in range(landsat.shape[-1])] + cov_columns + [\"lon\",\"lat\"]\n",
    "elif prepare_type == \"deep\":\n",
    "    cols = [f\"f{i:04}\" for i in range(features.shape[-1])] + [\"lon\",\"lat\"]\n",
    "\n",
    "cov = pd.DataFrame(np.concatenate(cov_list), columns=cols)\n",
    "display(cov)\n",
    "Y = pd.DataFrame(np.concatenate(y_list)).astype(int)\n",
    "os.makedirs(os.path.join(path_data, \"hmsc\"), exist_ok=True)\n",
    "# if prepare_type == \"orig\":\n",
    "#     cov.to_csv(os.path.join(path_data, \"hmsc\", \"train_cov.csv\"), index_label=False)\n",
    "# elif prepare_type == \"deep\":\n",
    "#     cov.to_csv(os.path.join(path_data, \"hmsc\", \"train_deepfeatures.csv\"), index_label=False)\n",
    "#Y.to_csv(os.path.join(path_data, \"hmsc\", \"train_Y.csv\"), index_label=False)\n",
    "surv = pd.DataFrame({\"surveyId\":np.concatenate(surv_list)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde6404-48fb-4f4d-8daa-35cb8fab7dbb",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37652cd9-388c-4734-a3aa-d2c7ca75e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path_sentinel = os.path.join(path_data, \"SatelitePatches/PA-test\")\n",
    "test_path_landsat = os.path.join(path_data, \"SateliteTimeSeries-Landsat/cubes/PA-test\")\n",
    "test_path_bioclim = os.path.join(path_data, \"BioclimTimeSeries/cubes/PA-test\")\n",
    "test_combined = read_test_data(path_data, cov_columns, cov_norm_coef, sel_countries)\n",
    "test_combined.reset_index(drop=True, inplace=True)\n",
    "test_dataset = TestDataset(test_path_sentinel, test_path_landsat, test_path_bioclim, test_combined, cov_columns, subset=\"test\", \n",
    "                           transform_sentinel=transform_sentinel, transform_landsat=transform_landsat, image_mean=image_mean, sentinel_mask_channel=sentinel_mask_channel)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "print(test_dataset[0][0].shape, test_dataset[0][1].shape, test_dataset[0][2].shape, test_dataset[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1d1f4-945c-4979-bf83-e3bf3fadc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_list, cov_list = [], []\n",
    "with torch.no_grad():\n",
    "    for sentinel, landsat, data_cov, lonlat, survey in tqdm(test_loader):\n",
    "        if prepare_type == \"orig\":\n",
    "            cov = torch.concat([sentinel, landsat.reshape([landsat.shape[0], -1]), data_cov, lonlat], dim=1).numpy()\n",
    "        elif prepare_type == \"deep\":\n",
    "            sentinel = sentinel.to(device)[:,:,None,:,:]\n",
    "            landsat, data_cov, lonlat = landsat.to(device), data_cov.to(device), lonlat.to(device)\n",
    "            features = model(sentinel, landsat, data_cov, lonlat)\n",
    "            cov = torch.concat([features, lonlat], dim=1).cpu().numpy()\n",
    "        cov_list.append(cov)\n",
    "        surv_list.append(survey.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdefbfc-c3f1-40a8-8263-75836e33f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = pd.DataFrame(np.concatenate(cov_list), columns=cols)\n",
    "display(cov)\n",
    "os.makedirs(os.path.join(path_data, \"hmsc\"), exist_ok=True)\n",
    "# if prepare_type == \"orig\":\n",
    "#     cov.to_csv(os.path.join(path_data, \"hmsc\", \"test_cov.csv\"), index_label=False)\n",
    "# elif prepare_type == \"deep\":\n",
    "#     cov.to_csv(os.path.join(path_data, \"hmsc\", \"test_deepfeatures.csv\"), index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea32548-7703-4c11-9cd8-64de3cebe0ad",
   "metadata": {},
   "source": [
    "### PO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c8073-f2f6-4838-91d7-5f7fa833cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"HOSTNAME\"] == \"gtbase\":\n",
    "    raise(Exception(\"Do not run PO data on local machine!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a7d22-0641-4656-b12c-313ec56ca47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_metadata_orig = pd.read_csv(os.path.join(path_data, \"GLC25_P0_metadata_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a495c3-4199-4b38-a0dc-8557d4b0b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_metadata = po_metadata_orig \n",
    "po_metadata = po_metadata.loc[:, [\"lat\", \"lon\", \"surveyId\"]].drop_duplicates().set_index(\"surveyId\", drop=True).sort_index()\n",
    "po_countries = pd.read_csv(os.path.join(path_data, \"po_with_countries.csv\"), index_col=0)\n",
    "po_metadata = po_metadata.join(po_countries.loc[~po_countries.index.duplicated(keep='first'), \"name\"])\n",
    "po_metadata = po_metadata.rename({\"name\": \"country\"}, axis=1)\n",
    "country_columns = [\"con\"+country[:3] for country in sel_countries] + [\"conOther\"]\n",
    "for country, col in zip(sel_countries, country_columns[:-1]):\n",
    "    po_metadata[col] = po_metadata[\"country\"] == country\n",
    "po_metadata[country_columns[-1]] = ~po_metadata[\"country\"].isin(sel_countries)\n",
    "po_worldcover = pd.read_csv(os.path.join(path_data, \"worldcover\", \"po_train_survey_points_with_worldcover.csv\"), index_col=0)\n",
    "po_combined = po_metadata.reset_index().merge(po_worldcover.loc[:,[\"lat\",\"lon\",\"class\"]], on=[\"lat\",\"lon\"]).set_index(\"surveyId\")\n",
    "po_combined[\"areaLog\"] = 0.0\n",
    "\n",
    "po_elevation = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"Elevation\", \"GLC25-PO-train-elevation.csv\"), index_col=0)\n",
    "po_combined = po_combined.join(po_elevation)\n",
    "\n",
    "po_soil = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"SoilGrids\", \"GLC25-PO-train-soilgrids.csv\"), index_col=0)\n",
    "for column in po_soil.columns: po_soil[column].fillna((po_soil[column].mean()), inplace=True)\n",
    "po_combined = po_combined.join(po_soil)\n",
    "\n",
    "po_wcdummy = pd.get_dummies(po_combined[\"class\"], prefix=\"wc\")\n",
    "po_wcdummy.drop(columns=[\"wc_70\", \"wc_100\"], inplace=True)\n",
    "po_combined = po_combined.join(po_wcdummy)\n",
    "\n",
    "po_landcover = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"LandCover\", \"GLC25-PO-train-landcover.csv\"), index_col=0)\n",
    "landcover_col_ind=[0,2,3,5,8,11,12]\n",
    "po_landcover = po_landcover.iloc[:, landcover_col_ind]\n",
    "po_combined = po_combined.join(po_landcover)\n",
    "\n",
    "po_snow = pd.read_csv(os.path.join(path_data, \"EnvironmentalValues\", \"chelsa_snow\", \"po_train_snowcover_chelsa_scd.csv\"), index_col=0).sort_index()\n",
    "po_snow = po_snow.rename({\"scd_1\": \"scd\"}, axis=1)\n",
    "po_snow = po_snow[~po_snow.index.duplicated(keep='first')]\n",
    "po_combined = po_combined.join(po_snow)\n",
    "\n",
    "po_combined.loc[:,cov_columns] = (po_combined.loc[:,cov_columns] - cov_norm_coef.loc[\"mean\"]) / cov_norm_coef.loc[\"std\"]\n",
    "po_combined.loc[:,cov_columns].isna().sum()\n",
    "po_combined.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdeafbd-11f4-4a5f-bfca-e25fd365c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_path_sentinel = os.path.join(os.environ['LOCAL_SCRATCH'], \"SatelitePatches/po/output/TIFF_64\")\n",
    "po_path_landsat = os.path.join(os.environ['LOCAL_SCRATCH'], \"SateliteTimeSeries-Landsat/cubes_landsat\")\n",
    "po_path_bioclim = os.path.join(os.environ['LOCAL_SCRATCH'], \"BioclimTimeSeries/cubes_bioclim\")\n",
    "po_dataset = TestDataset(po_path_sentinel, po_path_landsat, po_path_bioclim, po_combined, cov_columns, subset=\"po\", \n",
    "                           transform_sentinel=transform_sentinel, transform_landsat=transform_landsat, image_mean=image_mean, sentinel_mask_channel=sentinel_mask_channel)\n",
    "po_loader = DataLoader(po_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "print(po_dataset[0][0].shape, po_dataset[0][1].shape, po_dataset[0][2].shape, po_dataset[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3e308-d9bc-44b0-bc35-639315618272",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_list, cov_list = [], []\n",
    "df_count = 0\n",
    "counter = 0\n",
    "os.makedirs(os.path.join(path_save, \"hmsc\", \"po\"), exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for sentinel, landsat, data_cov, lonlat, survey in tqdm(po_loader):\n",
    "        if os.path.isfile(os.path.join(path_save, \"hmsc\", \"po\", f\"po_deepfeatures{df_count:03d}.feather\")):\n",
    "            counter += batch_size\n",
    "            if(counter >= 100000):\n",
    "                df_count += 1\n",
    "                counter = 0\n",
    "            continue\n",
    "            \n",
    "        if prepare_type == \"orig\":\n",
    "            cov = torch.concat([sentinel, landsat.reshape([landsat.shape[0], -1]), data_cov, lonlat], dim=1).numpy()\n",
    "        elif prepare_type == \"deep\":\n",
    "            sentinel = sentinel.to(device)[:,:,None,:,:]\n",
    "            landsat, data_cov, lonlat = landsat.to(device), data_cov.to(device), lonlat.to(device)\n",
    "            features = model(sentinel, landsat, data_cov, lonlat)\n",
    "            cov = torch.concat([features, lonlat], dim=1).cpu().numpy()\n",
    "        cov_list.append(cov)\n",
    "        surv_list.append(survey.numpy())\n",
    "        if(len(cov_list)*batch_size >= 100000):\n",
    "            cov = pd.DataFrame(np.concatenate(cov_list), columns=cols, index=np.concatenate(surv_list)).reset_index(drop=False)\n",
    "            if prepare_type == \"orig\":\n",
    "                cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_cov{df_count:03d}.feather\"))\n",
    "            elif prepare_type == \"deep\":\n",
    "                cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_deepfeatures{df_count:03d}.feather\"))\n",
    "            surv_list, cov_list = [], []\n",
    "            df_count += 1\n",
    "            gc.collect()\n",
    "\n",
    "if len(cov_list) > 0:\n",
    "    cov = pd.DataFrame(np.concatenate(cov_list), columns=cols, index=np.concatenate(surv_list)).reset_index(drop=False)\n",
    "    if prepare_type == \"orig\":\n",
    "        cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_cov{df_count:03d}.feather\"))\n",
    "    elif prepare_type == \"deep\":\n",
    "        cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_deepfeatures{df_count:03d}.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94affbe2-0e14-4940-9990-0a4fb19ab69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_list, cov_list = [], []\n",
    "df_count = 0\n",
    "counter = 0\n",
    "os.makedirs(os.path.join(path_save, \"hmsc\", \"po\"), exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for sentinel, landsat, data_cov, lonlat, survey in tqdm(po_loader):\n",
    "        if os.path.isfile(os.path.join(path_save, \"hmsc\", \"po\", f\"po_deepfeatures{df_count:03d}.feather\")):\n",
    "            counter += batch_size\n",
    "            if(counter >= 100000):\n",
    "                df_count += 1\n",
    "                counter = 0\n",
    "            continue\n",
    "            \n",
    "        if prepare_type == \"orig\":\n",
    "            cov = torch.concat([sentinel, landsat.reshape([landsat.shape[0], -1]), data_cov, lonlat], dim=1).numpy()\n",
    "        elif prepare_type == \"deep\":\n",
    "            sentinel = sentinel.to(device)[:,:,None,:,:]\n",
    "            landsat, data_cov, lonlat = landsat.to(device), data_cov.to(device), lonlat.to(device)\n",
    "            features = model(sentinel, landsat, data_cov, lonlat)\n",
    "            cov = torch.concat([features, lonlat], dim=1).cpu().numpy()\n",
    "        cov_list.append(cov)\n",
    "        surv_list.append(survey.numpy())\n",
    "        if(len(cov_list)*batch_size >= 100000):\n",
    "            cov = pd.DataFrame(np.concatenate(cov_list), columns=cols, index=np.concatenate(surv_list)).reset_index(drop=False)\n",
    "            if prepare_type == \"orig\":\n",
    "                cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_cov{df_count:03d}.feather\"))\n",
    "            elif prepare_type == \"deep\":\n",
    "                cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_deepfeatures{df_count:03d}.feather\"))\n",
    "            surv_list, cov_list = [], []\n",
    "            df_count += 1\n",
    "            gc.collect()\n",
    "\n",
    "if len(cov_list) > 0:\n",
    "    cov = pd.DataFrame(np.concatenate(cov_list), columns=cols, index=np.concatenate(surv_list)).reset_index(drop=False)\n",
    "    if prepare_type == \"orig\":\n",
    "        cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_cov{df_count:03d}.feather\"))\n",
    "    elif prepare_type == \"deep\":\n",
    "        cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_deepfeatures{df_count:03d}.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f75b0b-a9cf-4164-b380-f938c9210c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = pd.DataFrame(np.concatenate(cov_list), columns=cols, index=np.concatenate(surv_list)).reset_index(drop=False)\n",
    "cov.to_feather(os.path.join(path_save, \"hmsc\", \"po\", f\"po_cov{df_count:03d}.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ec842-ceab-4852-8c04-678c7769ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f\"sentinel{i}\" for i in range(sentinel.shape[-1])] + [f\"landsatbio{i}{j}\" for i in range(landsat.shape[-2]) for j in range(landsat.shape[-1])] + cov_columns + [\"lon\",\"lat\"]\n",
    "cov = pd.DataFrame(np.concatenate(cov_list), columns=cols, index=np.concatenate(surv_list))\n",
    "os.makedirs(os.path.join(path_save, \"hmsc\"), exist_ok=True)\n",
    "cov.to_csv(os.path.join(path_save, \"hmsc\", \"po_cov.csv\"), index_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1afe25-6eb4-45f1-9e24-62354818abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_metadata = po_metadata.join(po_soil)\n",
    "po_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe8f02-7b19-4a25-b186-1a02cfa792da",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_metadata = po_metadata_orig \n",
    "po_metadata = po_metadata.loc[:, [\"lat\", \"lon\", \"surveyId\"]].drop_duplicates().set_index(\"surveyId\", drop=True).sort_index()\n",
    "po_metadata = po_metadata.join(po_countries.loc[~po_countries.index.duplicated(keep='first'), \"name\"])\n",
    "po_metadata = po_metadata.rename({\"name\": \"country\"}, axis=1)\n",
    "for country, col in zip(sel_countries, country_columns[:-1]):\n",
    "    po_metadata[col] = po_metadata[\"country\"] == country\n",
    "po_metadata[country_columns[-1]] = ~po_metadata[\"country\"].isin(sel_countries)\n",
    "po_worldcover = pd.read_csv(os.path.join(path_data, \"worldcover\", \"po_train_survey_points_with_worldcover.csv\"), index_col=0)\n",
    "po_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72620669-232b-4ec7-9d0f-9da3117fce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_metadata = po_metadata.reset_index().merge(po_worldcover.loc[:,[\"lat\",\"lon\",\"class\"]], on=[\"lat\",\"lon\"]).set_index(\"surveyId\")\n",
    "po_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e62d4-78e6-4afe-ac21-6b035720c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "po_countries.loc[po_countries.name.isna(), \"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b2234-3565-415d-b986-585e890d74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import glc_datasets\n",
    "reload(glc_datasets)\n",
    "from glc_datasets import TrainDataset, TestDataset, read_train_data, read_test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a6f26-93ce-4eec-8883-79916c07b670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
